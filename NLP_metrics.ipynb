{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d4b89e",
   "metadata": {},
   "source": [
    "First let us gather the number of Males, Females and Unknowns in the dataset to gather some values beforehand. This will be used in talking about potential biases in news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5525510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per category:\n",
      "                              Male  Female  Unknown\n",
      "Text-Category                                      \n",
      "Business and Finance           134      20       13\n",
      "Crime and Justice              597     192       63\n",
      "Economy and Trade               22      13        5\n",
      "Education and Research          32       6        4\n",
      "Entertainment and Media        363     179       34\n",
      "Environment and Climate         48       9        8\n",
      "Health and Medicine            264      78       47\n",
      "Infrastructure and Transport    44       8        8\n",
      "Politics                        83      22        5\n",
      "Society and Culture            229     106       62\n",
      "Sports                          54      14       19\n",
      "Technology and Science          29      15        4\n",
      "\n",
      "Overall totals:\n",
      "Male:    1899\n",
      "Female:  662\n",
      "Unknown: 272\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read in your CSV\n",
    "df = pd.read_csv('thesis_NLP_component.csv')\n",
    "\n",
    "# 2. Strip any stray whitespace\n",
    "df['Ground truth'] = df['Ground truth'].str.strip()\n",
    "df['Text-Category'] = df['Text-Category'].str.strip()\n",
    "\n",
    "# 3. Extract the three counts into their own integer columns\n",
    "gt = df['Ground truth'].str.extract(\n",
    "    r'Male:\\s*(\\d+)\\s*Female:\\s*(\\d+)\\s*Unknown:\\s*(\\d+)',\n",
    "    expand=True\n",
    ")\n",
    "gt.columns = ['Male','Female','Unknown']\n",
    "gt = gt.astype(int)\n",
    "\n",
    "# 4. Combine with category and group\n",
    "counts = pd.concat([df['Text-Category'], gt], axis=1)\n",
    "per_category = counts.groupby('Text-Category').sum()\n",
    "\n",
    "# 5. Compute overall totals\n",
    "overall = per_category.sum()\n",
    "\n",
    "print(\"Counts per category:\")\n",
    "print(per_category)\n",
    "\n",
    "print(\"\\nOverall totals:\")\n",
    "print(f\"Male:    {overall['Male']}\")\n",
    "print(f\"Female:  {overall['Female']}\")\n",
    "print(f\"Unknown: {overall['Unknown']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb09e6",
   "metadata": {},
   "source": [
    "MAE (Mean Absolute Error): the average absolute difference between predicted and true counts per class per article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Deepseek vs Ground Truth): 0.3532\n",
      "MAE (NER vs Ground Truth):      0.8670\n",
      "MAE (Deepseek vs NER):          1.0115\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 1. Read the data\n",
    "df = pd.read_csv('thesis_NLP_component.csv')\n",
    "\n",
    "# 2. Normalise the headers by removing any stray whitespace\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2.1 Strip leading/trailing spaces from each count‐column\n",
    "for col in ['Deepseek:32b', 'Named entity recognition', 'Ground truth']:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# 3. Function to parse count strings like \"Male: 1 Female: 3 Unknown: 0\"\n",
    "def parse_counts(series):\n",
    "    counts = series.str.extract(\n",
    "        r'Male:\\s*(\\d+)\\s*Female:\\s*(\\d+)\\s*Unknown:\\s*(\\d+)',\n",
    "        expand=True\n",
    "    )\n",
    "    counts.columns = ['Male', 'Female', 'Unknown']\n",
    "    counts = counts.fillna(0)        # ← fill any non-matching rows with zeros\n",
    "    return counts.astype(int)        # now safe to cast without ValueError\n",
    "\n",
    "# 4. Parse each relevant column\n",
    "deepseek     = parse_counts(df['Deepseek:32b'])\n",
    "ground_truth = parse_counts(df['Ground truth'])\n",
    "ner          = parse_counts(df['Named entity recognition'])\n",
    "\n",
    "# 5. Function to compute MAE between two DataFrames of counts\n",
    "def compute_mae(pred, true):\n",
    "    abs_error = (pred - true).abs()\n",
    "    total_abs_error = abs_error.values.sum()\n",
    "    return total_abs_error / (len(df) * 3)  # 3 classes per article\n",
    "\n",
    "# 6. Calculate MAEs\n",
    "mae_deep_gt  = compute_mae(deepseek,     ground_truth)\n",
    "mae_ner_gt   = compute_mae(ner,          ground_truth)\n",
    "mae_deep_ner = compute_mae(deepseek,     ner)\n",
    "\n",
    "# 7. Display results\n",
    "print(f\"MAE (Deepseek vs Ground Truth): {mae_deep_gt:.4f}\")\n",
    "print(f\"MAE (NER vs Ground Truth):      {mae_ner_gt:.4f}\")\n",
    "print(f\"MAE (Deepseek vs NER):          {mae_deep_ner:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23178e",
   "metadata": {},
   "source": [
    "Now for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db89803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              MAE (Deepseek vs GT)  MAE (NER vs GT)  \\\n",
      "Text Category                                                         \n",
      "Business and Finance                      0.133333         0.287719   \n",
      "Crime and Justice                         0.366667         1.269444   \n",
      "Economy and Trade                         0.150000         0.666667   \n",
      "Education and Research                    0.066667         0.450000   \n",
      "Entertainment and Media                   0.431579         1.210526   \n",
      "Environment and Climate                   0.250000         0.466667   \n",
      "Health and Medicine                       0.936842         0.859649   \n",
      "Infrastructure and Transport              0.105263         0.684211   \n",
      "Politics                                  0.150000         0.850000   \n",
      "Society and Culture                       0.196491         0.877193   \n",
      "Sports                                    0.233333         1.083333   \n",
      "Technology and Science                    0.066667         0.550000   \n",
      "\n",
      "                              MAE (Deepseek vs NER)  \n",
      "Text Category                                        \n",
      "Business and Finance                       0.322807  \n",
      "Crime and Justice                          1.375000  \n",
      "Economy and Trade                          0.583333  \n",
      "Education and Research                     0.450000  \n",
      "Entertainment and Media                    1.256140  \n",
      "Environment and Climate                    0.683333  \n",
      "Health and Medicine                        1.564912  \n",
      "Infrastructure and Transport               0.649123  \n",
      "Politics                                   0.800000  \n",
      "Society and Culture                        0.912281  \n",
      "Sports                                     1.083333  \n",
      "Technology and Science                     0.583333  \n"
     ]
    }
   ],
   "source": [
    "# 1. Read and preprocess the data\n",
    "df = pd.read_csv('thesis_NLP_component.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Strip whitespace from the category column\n",
    "df['Text-Category'] = df['Text-Category'].str.strip()\n",
    "\n",
    "# Strip whitespace from count columns\n",
    "for col in ['Deepseek:32b', 'Named entity recognition', 'Ground truth']:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# 2. Function to parse the count strings\n",
    "def parse_counts(series):\n",
    "    counts = series.str.extract(\n",
    "        r'Male:\\s*(\\d+)\\s*Female:\\s*(\\d+)\\s*Unknown:\\s*(\\d+)',\n",
    "        expand=True\n",
    "    )\n",
    "    counts.columns = ['Male', 'Female', 'Unknown']\n",
    "    return counts.fillna(0).astype(int)\n",
    "\n",
    "# 3. Function to compute MAE between two DataFrames of counts\n",
    "def compute_mae(pred, true):\n",
    "    abs_error = (pred - true).abs()\n",
    "    return abs_error.values.sum() / (len(pred) * 3)\n",
    "\n",
    "# 4. Parse each relevant column\n",
    "deepseek     = parse_counts(df['Deepseek:32b'])\n",
    "ner          = parse_counts(df['Named entity recognition'])\n",
    "ground_truth = parse_counts(df['Ground truth'])\n",
    "\n",
    "# 5. Compute MAE per category\n",
    "results = []\n",
    "for category, idx in df.groupby('Text-Category').groups.items():\n",
    "    ds = deepseek.loc[idx]\n",
    "    nr = ner.loc[idx]\n",
    "    gt = ground_truth.loc[idx]\n",
    "    \n",
    "    results.append({\n",
    "        'Text Category': category,\n",
    "        'MAE (Deepseek vs GT)': compute_mae(ds, gt),\n",
    "        'MAE (NER vs GT)':      compute_mae(nr, gt),\n",
    "        'MAE (Deepseek vs NER)':compute_mae(ds, nr)\n",
    "    })\n",
    "\n",
    "# 6. Build and display a DataFrame\n",
    "mae_by_category = pd.DataFrame(results).set_index('Text Category')\n",
    "print(mae_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fccf18",
   "metadata": {},
   "source": [
    "This is for gender breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e05eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Deepseek vs Ground Truth):\n",
      "  Male:    0.3067\n",
      "  Female:  0.1440\n",
      "  Unknown: 0.6088\n",
      "\n",
      "MAE (NER vs Ground Truth):\n",
      "  Male:    1.2676\n",
      "  Female:  0.7700\n",
      "  Unknown: 0.5634\n",
      "\n",
      "MAE (Deepseek vs NER):\n",
      "  Male:    1.2958\n",
      "  Female:  0.7355\n",
      "  Unknown: 1.0031\n"
     ]
    }
   ],
   "source": [
    "# 1. Absolute errors for each pairing\n",
    "err_ds_gt = (deepseek - ground_truth).abs()\n",
    "err_ner_gt = (ner       - ground_truth).abs()\n",
    "err_ds_ner = (deepseek  - ner).abs()\n",
    "\n",
    "# 2. Compute per-class MAE for each pairing\n",
    "mae_ds_gt_male    = err_ds_gt['Male'].mean()\n",
    "mae_ds_gt_female  = err_ds_gt['Female'].mean()\n",
    "mae_ds_gt_unknown = err_ds_gt['Unknown'].mean()\n",
    "\n",
    "mae_ner_gt_male    = err_ner_gt['Male'].mean()\n",
    "mae_ner_gt_female  = err_ner_gt['Female'].mean()\n",
    "mae_ner_gt_unknown = err_ner_gt['Unknown'].mean()\n",
    "\n",
    "mae_ds_ner_male    = err_ds_ner['Male'].mean()\n",
    "mae_ds_ner_female  = err_ds_ner['Female'].mean()\n",
    "mae_ds_ner_unknown = err_ds_ner['Unknown'].mean()\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"MAE (Deepseek vs Ground Truth):\")\n",
    "print(f\"  Male:    {mae_ds_gt_male:.4f}\")\n",
    "print(f\"  Female:  {mae_ds_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {mae_ds_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"MAE (NER vs Ground Truth):\")\n",
    "print(f\"  Male:    {mae_ner_gt_male:.4f}\")\n",
    "print(f\"  Female:  {mae_ner_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {mae_ner_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"MAE (Deepseek vs NER):\")\n",
    "print(f\"  Male:    {mae_ds_ner_male:.4f}\")\n",
    "print(f\"  Female:  {mae_ds_ner_female:.4f}\")\n",
    "print(f\"  Unknown: {mae_ds_ner_unknown:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c17aa",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error): the average of the squared differences between predicted and true counts, penalising larger errors more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "964c83de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Deepseek vs Ground Truth): 21.5206\n",
      "MSE (NER vs Ground Truth):      4.4799\n",
      "MSE (Deepseek vs NER):          25.5477\n"
     ]
    }
   ],
   "source": [
    "mse_deep_gt = ((deepseek - ground_truth) ** 2).values.sum() / (len(df) * 3)\n",
    "mse_ner_gt  = ((ner - ground_truth)       ** 2).values.sum() / (len(df) * 3)\n",
    "mse_deep_ner= ((deepseek - ner)           ** 2).values.sum() / (len(df) * 3)\n",
    "\n",
    "print(f\"MSE (Deepseek vs Ground Truth): {mse_deep_gt:.4f}\")\n",
    "print(f\"MSE (NER vs Ground Truth):      {mse_ner_gt:.4f}\")\n",
    "print(f\"MSE (Deepseek vs NER):          {mse_deep_ner:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80d6ee",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error): the square root of MSE, giving error in the same units as the original counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4c20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Deepseek vs Ground Truth): 4.6390\n",
      "RMSE (NER vs Ground Truth):      2.1166\n",
      "RMSE (Deepseek vs NER):          5.0545\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "rmse_deep_gt = math.sqrt(mse_deep_gt)\n",
    "rmse_ner_gt  = math.sqrt(mse_ner_gt)\n",
    "rmse_deep_ner= math.sqrt(mse_deep_ner)\n",
    "\n",
    "print(f\"RMSE (Deepseek vs Ground Truth): {rmse_deep_gt:.4f}\")\n",
    "print(f\"RMSE (NER vs Ground Truth):      {rmse_ner_gt:.4f}\")\n",
    "print(f\"RMSE (Deepseek vs NER):          {rmse_deep_ner:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2fa5c",
   "metadata": {},
   "source": [
    "Now for model performance across categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da438b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              MSE (Deepseek vs GT)  MSE (NER vs GT)  \\\n",
      "Text Category                                                         \n",
      "Business and Finance                      0.217544         0.547368   \n",
      "Crime and Justice                         0.783333         7.269444   \n",
      "Economy and Trade                         0.183333         1.933333   \n",
      "Education and Research                    0.066667         0.983333   \n",
      "Entertainment and Media                   1.400000         7.484211   \n",
      "Environment and Climate                   2.450000         1.733333   \n",
      "Health and Medicine                     141.112281         3.715789   \n",
      "Infrastructure and Transport              0.245614         1.280702   \n",
      "Politics                                  0.150000         2.583333   \n",
      "Society and Culture                       0.308772         6.561404   \n",
      "Sports                                    0.300000         3.083333   \n",
      "Technology and Science                    0.066667         1.016667   \n",
      "\n",
      "                              MSE (Deepseek vs NER)  RMSE (Deepseek vs GT)  \\\n",
      "Text Category                                                                \n",
      "Business and Finance                       0.645614               0.466416   \n",
      "Crime and Justice                          7.947222               0.885061   \n",
      "Economy and Trade                          1.383333               0.428174   \n",
      "Education and Research                     0.883333               0.258199   \n",
      "Entertainment and Media                    6.568421               1.183216   \n",
      "Environment and Climate                    5.350000               1.565248   \n",
      "Health and Medicine                      144.821053              11.879069   \n",
      "Infrastructure and Transport               1.315789               0.495595   \n",
      "Politics                                   2.300000               0.387298   \n",
      "Society and Culture                        6.498246               0.555673   \n",
      "Sports                                     3.250000               0.547723   \n",
      "Technology and Science                     1.116667               0.258199   \n",
      "\n",
      "                              RMSE (NER vs GT)  RMSE (Deepseek vs NER)  \n",
      "Text Category                                                           \n",
      "Business and Finance                  0.739844                0.803501  \n",
      "Crime and Justice                     2.696191                2.819082  \n",
      "Economy and Trade                     1.390444                1.176152  \n",
      "Education and Research                0.991632                0.939858  \n",
      "Entertainment and Media               2.735729                2.562893  \n",
      "Environment and Climate               1.316561                2.313007  \n",
      "Health and Medicine                   1.927638               12.034162  \n",
      "Infrastructure and Transport          1.131681                1.147079  \n",
      "Politics                              1.607275                1.516575  \n",
      "Society and Culture                   2.561524                2.549166  \n",
      "Sports                                1.755942                1.802776  \n",
      "Technology and Science                1.008299                1.056724  \n"
     ]
    }
   ],
   "source": [
    "# 1. Read and preprocess the data\n",
    "df = pd.read_csv('thesis_NLP_component.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "df['Text-Category'] = df['Text-Category'].str.strip()\n",
    "for col in ['Deepseek:32b', 'Named entity recognition', 'Ground truth']:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# 2. Parse counts helper\n",
    "def parse_counts(series):\n",
    "    counts = series.str.extract(\n",
    "        r'Male:\\s*(\\d+)\\s*Female:\\s*(\\d+)\\s*Unknown:\\s*(\\d+)',\n",
    "        expand=True\n",
    "    )\n",
    "    counts.columns = ['Male','Female','Unknown']\n",
    "    return counts.fillna(0).astype(int)\n",
    "\n",
    "deepseek     = parse_counts(df['Deepseek:32b'])\n",
    "ner          = parse_counts(df['Named entity recognition'])\n",
    "ground_truth = parse_counts(df['Ground truth'])\n",
    "\n",
    "# 3. Compute per-category MSE and RMSE\n",
    "results = []\n",
    "for category, idx in df.groupby('Text-Category').groups.items():\n",
    "    ds = deepseek.loc[idx]\n",
    "    nr = ner.loc[idx]\n",
    "    gt = ground_truth.loc[idx]\n",
    "    n = len(idx) * 3\n",
    "    \n",
    "    # MSEs\n",
    "    mse_ds_gt  = ((ds  - gt)**2).values.sum()  / n\n",
    "    mse_ner_gt = ((nr  - gt)**2).values.sum()  / n\n",
    "    mse_ds_ner = ((ds  - nr)**2).values.sum()  / n\n",
    "    \n",
    "    # RMSEs\n",
    "    rmse_ds_gt  = math.sqrt(mse_ds_gt)\n",
    "    rmse_ner_gt = math.sqrt(mse_ner_gt)\n",
    "    rmse_ds_ner = math.sqrt(mse_ds_ner)\n",
    "    \n",
    "    results.append({\n",
    "        'Text Category': category,\n",
    "        'MSE (Deepseek vs GT)':  mse_ds_gt,\n",
    "        'MSE (NER vs GT)':       mse_ner_gt,\n",
    "        'MSE (Deepseek vs NER)': mse_ds_ner,\n",
    "        'RMSE (Deepseek vs GT)':  rmse_ds_gt,\n",
    "        'RMSE (NER vs GT)':       rmse_ner_gt,\n",
    "        'RMSE (Deepseek vs NER)': rmse_ds_ner,\n",
    "    })\n",
    "\n",
    "# 4. Build and display\n",
    "metrics_by_category = pd.DataFrame(results).set_index('Text Category')\n",
    "print(metrics_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5190a",
   "metadata": {},
   "source": [
    "Gender breakdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218f6819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Deepseek vs Ground Truth):\n",
      "  Male:    0.7856\n",
      "  Female:  0.2848\n",
      "  Unknown: 63.4914\n",
      "\n",
      "RMSE (Deepseek vs Ground Truth):\n",
      "  Male:    0.8863\n",
      "  Female:  0.5337\n",
      "  Unknown: 7.9681\n",
      "\n",
      "MSE (NER vs Ground Truth):\n",
      "  Male:    6.2097\n",
      "  Female:  2.8858\n",
      "  Unknown: 4.3443\n",
      "\n",
      "RMSE (NER vs Ground Truth):\n",
      "  Male:    2.4919\n",
      "  Female:  1.6988\n",
      "  Unknown: 2.0843\n",
      "\n",
      "MSE (Deepseek vs NER):\n",
      "  Male:    6.2535\n",
      "  Female:  2.4632\n",
      "  Unknown: 67.9264\n",
      "\n",
      "RMSE (Deepseek vs NER):\n",
      "  Male:    2.5007\n",
      "  Female:  1.5695\n",
      "  Unknown: 8.2418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Squared errors for each pairing\n",
    "sq_err_ds_gt = (deepseek - ground_truth) ** 2\n",
    "sq_err_ner_gt = (ner       - ground_truth) ** 2\n",
    "sq_err_ds_ner = (deepseek  - ner)          ** 2\n",
    "\n",
    "# 2. Compute per-class MSE (mean of squared errors)\n",
    "mse_ds_gt_male    = sq_err_ds_gt['Male'].mean()\n",
    "mse_ds_gt_female  = sq_err_ds_gt['Female'].mean()\n",
    "mse_ds_gt_unknown = sq_err_ds_gt['Unknown'].mean()\n",
    "\n",
    "mse_ner_gt_male    = sq_err_ner_gt['Male'].mean()\n",
    "mse_ner_gt_female  = sq_err_ner_gt['Female'].mean()\n",
    "mse_ner_gt_unknown = sq_err_ner_gt['Unknown'].mean()\n",
    "\n",
    "mse_ds_ner_male    = sq_err_ds_ner['Male'].mean()\n",
    "mse_ds_ner_female  = sq_err_ds_ner['Female'].mean()\n",
    "mse_ds_ner_unknown = sq_err_ds_ner['Unknown'].mean()\n",
    "\n",
    "# 3. Compute per-class RMSE\n",
    "rmse_ds_gt_male    = np.sqrt(mse_ds_gt_male)\n",
    "rmse_ds_gt_female  = np.sqrt(mse_ds_gt_female)\n",
    "rmse_ds_gt_unknown = np.sqrt(mse_ds_gt_unknown)\n",
    "\n",
    "rmse_ner_gt_male    = np.sqrt(mse_ner_gt_male)\n",
    "rmse_ner_gt_female  = np.sqrt(mse_ner_gt_female)\n",
    "rmse_ner_gt_unknown = np.sqrt(mse_ner_gt_unknown)\n",
    "\n",
    "rmse_ds_ner_male    = np.sqrt(mse_ds_ner_male)\n",
    "rmse_ds_ner_female  = np.sqrt(mse_ds_ner_female)\n",
    "rmse_ds_ner_unknown = np.sqrt(mse_ds_ner_unknown)\n",
    "\n",
    "# 4. Display the results\n",
    "print(\"MSE (Deepseek vs Ground Truth):\")\n",
    "print(f\"  Male:    {mse_ds_gt_male:.4f}\")\n",
    "print(f\"  Female:  {mse_ds_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {mse_ds_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"RMSE (Deepseek vs Ground Truth):\")\n",
    "print(f\"  Male:    {rmse_ds_gt_male:.4f}\")\n",
    "print(f\"  Female:  {rmse_ds_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {rmse_ds_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"MSE (NER vs Ground Truth):\")\n",
    "print(f\"  Male:    {mse_ner_gt_male:.4f}\")\n",
    "print(f\"  Female:  {mse_ner_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {mse_ner_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"RMSE (NER vs Ground Truth):\")\n",
    "print(f\"  Male:    {rmse_ner_gt_male:.4f}\")\n",
    "print(f\"  Female:  {rmse_ner_gt_female:.4f}\")\n",
    "print(f\"  Unknown: {rmse_ner_gt_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"MSE (Deepseek vs NER):\")\n",
    "print(f\"  Male:    {mse_ds_ner_male:.4f}\")\n",
    "print(f\"  Female:  {mse_ds_ner_female:.4f}\")\n",
    "print(f\"  Unknown: {mse_ds_ner_unknown:.4f}\\n\")\n",
    "\n",
    "print(\"RMSE (Deepseek vs NER):\")\n",
    "print(f\"  Male:    {rmse_ds_ner_male:.4f}\")\n",
    "print(f\"  Female:  {rmse_ds_ner_female:.4f}\")\n",
    "print(f\"  Unknown: {rmse_ds_ner_unknown:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e8798",
   "metadata": {},
   "source": [
    "This cell was used to check if all data was correct after analysis (Extra check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80922573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No format mismatches detected.\n"
     ]
    }
   ],
   "source": [
    "# Read data and normalise headers\n",
    "df = pd.read_csv('thesis_NLP_component.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Strip leading/trailing spaces from each count‐column\n",
    "for col in ['Deepseek:32b', 'Named entity recognition', 'Ground truth']:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Pattern for \"Male: X Female: Y Unknown: Z\"\n",
    "pattern = r'Male:\\s*(\\d+)\\s*Female:\\s*(\\d+)\\s*Unknown:\\s*(\\d+)'\n",
    "\n",
    "mismatches = {}\n",
    "for col in ['Deepseek:32b', 'Named entity recognition', 'Ground truth']:\n",
    "    mask = ~df[col].str.match(pattern, na=False)\n",
    "    issues = df.loc[mask, ['Text-Link', col]]\n",
    "    if not issues.empty:\n",
    "        mismatches[col] = issues\n",
    "\n",
    "if mismatches:\n",
    "    for col, issues in mismatches.items():\n",
    "        print(f\"\\nColumn '{col}' has {len(issues)} mismatched rows:\")\n",
    "        print(issues.to_string(index=False))\n",
    "else:\n",
    "    print(\"No format mismatches detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
